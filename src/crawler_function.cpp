#include "crawler_function.hpp"
#include "crawler_utils.hpp"
#include "thread_utils.hpp"
#include "robots_parser.hpp"
#include "sitemap_parser.hpp"
#include "http_client.hpp"
#include "link_parser.hpp"
#include "structured_data.hpp"
#include "json_path_evaluator.hpp"
#include "crawl_parser.hpp"
#include "duckdb/function/table_function.hpp"
#include "duckdb/common/string_util.hpp"
#include "duckdb/main/connection.hpp"
#include "duckdb/main/client_context.hpp"
#include "duckdb/main/extension/extension_loader.hpp"
#include "duckdb/main/appender.hpp"
#include <set>

#include <algorithm>
#include <atomic>
#include <csignal>
#include <chrono>
#include <thread>
#include <mutex>
#include <condition_variable>
#include <sstream>
#include <unordered_map>
#include <queue>
#include <deque>
#include <vector>
#include <ctime>
#include <cmath>
#include <future>
#include <zlib.h>

namespace duckdb {

// CrawlErrorType, ErrorTypeToString, ClassifyError defined in crawler_utils.hpp

// Load HTTP settings from DuckDB configuration
static void LoadHttpSettingsFromDuckDB(Connection &conn) {
	HttpSettings settings;

	auto result = conn.Query(R"(
		SELECT
			current_setting('http_timeout')::INT as timeout,
			current_setting('http_keep_alive')::BOOL as keep_alive,
			current_setting('http_proxy') as proxy,
			current_setting('http_proxy_username') as proxy_user,
			current_setting('http_proxy_password') as proxy_pass
	)");

	if (!result->HasError() && result->RowCount() > 0) {
		auto chunk = result->Fetch();
		if (chunk && chunk->size() > 0) {
			auto timeout_val = chunk->GetValue(0, 0);
			if (!timeout_val.IsNull()) {
				settings.timeout_seconds = timeout_val.GetValue<int>();
			}

			auto keep_alive_val = chunk->GetValue(1, 0);
			if (!keep_alive_val.IsNull()) {
				settings.keep_alive = keep_alive_val.GetValue<bool>();
			}

			auto proxy_val = chunk->GetValue(2, 0);
			if (!proxy_val.IsNull()) {
				settings.proxy = StringValue::Get(proxy_val);
			}

			auto proxy_user_val = chunk->GetValue(3, 0);
			if (!proxy_user_val.IsNull()) {
				settings.proxy_username = StringValue::Get(proxy_user_val);
			}

			auto proxy_pass_val = chunk->GetValue(4, 0);
			if (!proxy_pass_val.IsNull()) {
				settings.proxy_password = StringValue::Get(proxy_pass_val);
			}
		}
	}

	SetHttpSettings(settings);
}

// DecompressGzip, IsGzippedData defined in crawler_utils.hpp

// Global signal flag for graceful shutdown
static std::atomic<bool> g_shutdown_requested(false);
static std::atomic<int> g_sigint_count(0);
static std::chrono::steady_clock::time_point g_last_sigint_time;

// Global connection counter for rate limiting across all domains
static std::atomic<int> g_active_connections(0);

// Signal handler
static void SignalHandler(int signum) {
	if (signum == SIGINT) {
		auto now = std::chrono::steady_clock::now();
		auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(now - g_last_sigint_time).count();

		g_sigint_count++;
		g_last_sigint_time = now;

		if (g_sigint_count >= 2 && elapsed < 3) {
			// Double Ctrl+C within 3 seconds - force exit
			std::exit(1);
		}

		g_shutdown_requested = true;
	}
}

// DomainState, UrlQueueEntry, ThreadSafeUrlQueue, ThreadSafeDomainMap
// are defined in thread_utils.hpp

// FibonacciBackoffSeconds defined in crawler_utils.hpp

// Adaptive rate limiting: adjust delay based on response times
// Uses exponential moving average (EMA) with alpha=0.2
static void UpdateAdaptiveDelay(DomainState &state, double response_ms, double max_delay) {
	// Update EMA (alpha=0.2 for smoothing)
	constexpr double alpha = 0.2;
	if (state.response_count == 0) {
		state.average_response_ms = response_ms;
	} else {
		state.average_response_ms = alpha * response_ms + (1.0 - alpha) * state.average_response_ms;
	}
	state.response_count++;

	// Need at least 3 samples before adapting
	if (state.response_count < 3) {
		return;
	}

	// If response is significantly slower than average, increase delay
	if (response_ms > 2.0 * state.average_response_ms) {
		state.crawl_delay_seconds = std::min(state.crawl_delay_seconds * 1.5, max_delay);
	}
	// If response is significantly faster than average, decrease delay (but respect floor)
	else if (response_ms < 0.5 * state.average_response_ms) {
		state.crawl_delay_seconds = std::max(state.crawl_delay_seconds * 0.9, state.min_crawl_delay_seconds);
	}
}

// ParseAndValidateServerDate defined in crawler_utils.hpp

//===--------------------------------------------------------------------===//
// crawl_into_internal - Used by CRAWL CONCURRENTLY parser extension
//===--------------------------------------------------------------------===//

struct CrawlIntoBindData : public TableFunctionData {
	int statement_type = 0;       // 0=CRAWL
	std::string source_query;
	std::string target_table;
	std::string user_agent;
	double default_crawl_delay = 1.0;
	double min_crawl_delay = 0.0;
	double max_crawl_delay = 60.0;
	int timeout_seconds = 30;
	bool respect_robots_txt = true;
	bool log_skipped = true;
	std::string url_filter;       // LIKE pattern for filtering URLs
	double sitemap_cache_hours = 24.0;  // Hours to cache sitemap discovery results
	bool update_stale = false;    // Re-crawl stale URLs based on sitemap lastmod/changefreq
	int max_retry_backoff_seconds = 600;   // Max Fibonacci backoff for 429/5XX/timeout
	int max_parallel_per_domain = 8;       // Max concurrent requests per domain (if no crawl-delay)
	int max_total_connections = 32;        // Global max concurrent connections
	int64_t max_response_bytes = 10 * 1024 * 1024;  // Max response size (10MB)
	bool compress = true;                  // Request gzip compression
	std::string accept_content_types;      // Only accept these content types
	std::string reject_content_types;      // Reject these content types
	// Link-following fallback
	bool follow_links = false;
	bool allow_subdomains = false;
	int max_crawl_pages = 10000;
	int max_crawl_depth = 10;
	bool respect_nofollow = true;
	bool follow_canonical = true;
	// Multi-threading
	int num_threads = 0;  // 0 = auto (hardware_concurrency)
	// Structured data extraction
	bool extract_js = true;  // Extract JS variables (can be disabled for performance)
	// EXTRACT clause - custom column extraction
	std::string extract_specs_json;  // JSON serialized extract specs
	std::vector<ExtractSpec> extract_specs;  // Parsed extract specs
	std::vector<JsonPath> extract_paths;  // Parsed JSON paths for each spec
};

struct CrawlIntoGlobalState : public GlobalTableFunctionState {
	std::mutex mutex;
	bool executed = false;
	int64_t rows_inserted = 0;

	// Progress tracking
	std::atomic<bool> discovery_started{false};
	std::atomic<int64_t> total_urls{0};
	std::atomic<int64_t> processed_urls{0};

	idx_t MaxThreads() const override {
		return 1;
	}
};

static unique_ptr<FunctionData> CrawlIntoBind(ClientContext &context, TableFunctionBindInput &input,
                                               vector<LogicalType> &return_types, vector<string> &names) {
	auto bind_data = make_uniq<CrawlIntoBindData>();

	// Get parameters from positional arguments (set by PlanCrawl)
	// Order: statement_type, source_query, target_table, user_agent, delays..., url_filter, sitemap_cache_hours,
	//        update_stale, max_retry_backoff_seconds, max_parallel_per_domain, max_total_connections, max_response_bytes,
	//        compress, accept_content_types, reject_content_types, follow_links, allow_subdomains, max_crawl_pages,
	//        max_crawl_depth, respect_nofollow, follow_canonical, num_threads, extract_js
	if (input.inputs.size() >= 29) {
		bind_data->statement_type = input.inputs[0].GetValue<int32_t>();
		bind_data->source_query = StringValue::Get(input.inputs[1]);
		bind_data->target_table = StringValue::Get(input.inputs[2]);
		bind_data->user_agent = StringValue::Get(input.inputs[3]);
		bind_data->default_crawl_delay = input.inputs[4].GetValue<double>();
		bind_data->min_crawl_delay = input.inputs[5].GetValue<double>();
		bind_data->max_crawl_delay = input.inputs[6].GetValue<double>();
		bind_data->timeout_seconds = input.inputs[7].GetValue<int>();
		bind_data->respect_robots_txt = input.inputs[8].GetValue<bool>();
		bind_data->log_skipped = input.inputs[9].GetValue<bool>();
		bind_data->url_filter = StringValue::Get(input.inputs[10]);
		bind_data->sitemap_cache_hours = input.inputs[11].GetValue<double>();
		bind_data->update_stale = input.inputs[12].GetValue<bool>();
		bind_data->max_retry_backoff_seconds = input.inputs[13].GetValue<int>();
		bind_data->max_parallel_per_domain = input.inputs[14].GetValue<int>();
		bind_data->max_total_connections = input.inputs[15].GetValue<int>();
		bind_data->max_response_bytes = input.inputs[16].GetValue<int64_t>();
		bind_data->compress = input.inputs[17].GetValue<bool>();
		bind_data->accept_content_types = StringValue::Get(input.inputs[18]);
		bind_data->reject_content_types = StringValue::Get(input.inputs[19]);
		bind_data->follow_links = input.inputs[20].GetValue<bool>();
		bind_data->allow_subdomains = input.inputs[21].GetValue<bool>();
		bind_data->max_crawl_pages = input.inputs[22].GetValue<int>();
		bind_data->max_crawl_depth = input.inputs[23].GetValue<int>();
		bind_data->respect_nofollow = input.inputs[24].GetValue<bool>();
		bind_data->follow_canonical = input.inputs[25].GetValue<bool>();
		bind_data->num_threads = input.inputs[26].GetValue<int>();
		bind_data->extract_js = input.inputs[27].GetValue<bool>();
		bind_data->extract_specs_json = StringValue::Get(input.inputs[28]);

		// Parse extract_specs_json into extract_specs and extract_paths
		if (!bind_data->extract_specs_json.empty() && bind_data->extract_specs_json != "[]") {
			// Simple JSON parsing for extract specs
			// Format: [{"expr":"...","alias":"...","is_text":true/false},...]
			std::string json = bind_data->extract_specs_json;
			size_t pos = 0;
			while ((pos = json.find("{\"expr\":", pos)) != std::string::npos) {
				ExtractSpec spec;

				// Extract expr
				size_t expr_start = json.find(":\"", pos) + 2;
				size_t expr_end = json.find("\",\"alias\"", expr_start);
				if (expr_end != std::string::npos) {
					spec.expression = json.substr(expr_start, expr_end - expr_start);
					// Unescape quotes
					size_t quote_pos = 0;
					while ((quote_pos = spec.expression.find("\\\"", quote_pos)) != std::string::npos) {
						spec.expression.replace(quote_pos, 2, "\"");
						quote_pos++;
					}
				}

				// Extract alias
				size_t alias_start = json.find("\"alias\":\"", pos);
				if (alias_start != std::string::npos) {
					alias_start += 9;
					size_t alias_end = json.find("\",\"is_text\"", alias_start);
					if (alias_end != std::string::npos) {
						spec.alias = json.substr(alias_start, alias_end - alias_start);
					}
				}

				// Extract is_text
				size_t is_text_pos = json.find("\"is_text\":", pos);
				if (is_text_pos != std::string::npos) {
					spec.is_text = (json.find("true", is_text_pos) < json.find("}", is_text_pos));
				}

				bind_data->extract_specs.push_back(spec);
				bind_data->extract_paths.push_back(ParseJsonPath(spec.expression));
				pos++;
			}
		}
	} else {
		throw BinderException("crawl_into_internal: insufficient parameters (expected 29, got " +
		                      std::to_string(input.inputs.size()) + ")");
	}

	// Validate table name for SQL safety
	if (!IsValidSqlIdentifier(bind_data->target_table)) {
		throw BinderException("Invalid table name: '" + bind_data->target_table +
		                      "'. Table names must start with letter/underscore and contain only "
		                      "alphanumeric characters, underscores, or periods (for schema.table)");
	}

	// Return schema: just count of rows changed
	names.emplace_back("Count");
	return_types.emplace_back(LogicalType::BIGINT);

	return std::move(bind_data);
}

static unique_ptr<GlobalTableFunctionState> CrawlIntoInitGlobal(ClientContext &context,
                                                                  TableFunctionInitInput &input) {
	auto state = make_uniq<CrawlIntoGlobalState>();

	// Reset signal state
	g_shutdown_requested = false;
	g_sigint_count = 0;

	// Install signal handler
	std::signal(SIGINT, SignalHandler);

	return std::move(state);
}

// Helper: Check if URL matches LIKE pattern (simple SQL LIKE implementation)
static bool MatchesLikePattern(const std::string &url, const std::string &pattern) {
	if (pattern.empty()) {
		return true;
	}

	// Convert SQL LIKE pattern to simple matching
	// % = any characters, _ = single character
	size_t url_pos = 0;
	size_t pat_pos = 0;
	size_t url_len = url.length();
	size_t pat_len = pattern.length();

	size_t star_pos = std::string::npos;
	size_t match_pos = 0;

	while (url_pos < url_len) {
		if (pat_pos < pat_len && (pattern[pat_pos] == url[url_pos] || pattern[pat_pos] == '_')) {
			url_pos++;
			pat_pos++;
		} else if (pat_pos < pat_len && pattern[pat_pos] == '%') {
			star_pos = pat_pos;
			match_pos = url_pos;
			pat_pos++;
		} else if (star_pos != std::string::npos) {
			pat_pos = star_pos + 1;
			match_pos++;
			url_pos = match_pos;
		} else {
			return false;
		}
	}

	while (pat_pos < pat_len && pattern[pat_pos] == '%') {
		pat_pos++;
	}

	return pat_pos == pat_len;
}

// Helper: Ensure sitemap cache table exists with SURT-based indexing
static void EnsureSitemapCacheTable(Connection &conn) {
	// Check if table exists and has old schema (hostname or domain_surt column)
	auto check_result = conn.Query("SELECT 1 FROM _crawl_sitemap_cache LIMIT 0");
	if (!check_result->HasError()) {
		// Table exists - check if it has the old schema
		auto col_check = conn.Query("SELECT hostname FROM _crawl_sitemap_cache LIMIT 0");
		auto col_check2 = conn.Query("SELECT domain_surt FROM _crawl_sitemap_cache LIMIT 0");
		if (!col_check->HasError() || !col_check2->HasError()) {
			// Old schema - drop and recreate
			conn.Query("DROP TABLE _crawl_sitemap_cache");
		}
	}

	// Create table with SURT-based schema (query by surt_key LIKE 'fi,normal)%')
	conn.Query("CREATE TABLE IF NOT EXISTS _crawl_sitemap_cache ("
	           "surt_key VARCHAR PRIMARY KEY, "  // SURT of URL, e.g. "fi,normal)/path"
	           "url VARCHAR NOT NULL, "
	           "lastmod TIMESTAMP, "
	           "changefreq VARCHAR, "
	           "priority DOUBLE, "
	           "discovered_at TIMESTAMP DEFAULT NOW())");
}

// Helper: Convert changefreq to hours threshold
static double ChangefreqToHours(const std::string &changefreq) {
	if (changefreq.empty()) {
		return 168.0;  // Default: weekly
	}
	std::string freq = StringUtil::Lower(changefreq);
	if (freq == "always") return 0.0;
	if (freq == "hourly") return 1.0;
	if (freq == "daily") return 24.0;
	if (freq == "weekly") return 168.0;
	if (freq == "monthly") return 720.0;
	if (freq == "yearly") return 8760.0;
	if (freq == "never") return 87600.0;  // ~10 years
	return 168.0;  // Default: weekly
}

// Helper: Check if URL is stale based on sitemap lastmod or changefreq
// Returns true if the URL should be re-crawled
static bool IsUrlStale(Connection &conn, const std::string &url, const std::string &target_table) {
	// Get crawled_at from target table
	auto crawled_result = conn.Query(
	    "SELECT crawled_at FROM " + QuoteSqlIdentifier(target_table) + " WHERE url = $1", url);

	if (crawled_result->HasError()) {
		return false;  // Can't determine, assume not stale
	}

	auto crawled_chunk = crawled_result->Fetch();
	if (!crawled_chunk || crawled_chunk->size() == 0) {
		return false;  // URL not found in target, not a stale update
	}

	auto crawled_val = crawled_chunk->GetValue(0, 0);
	if (crawled_val.IsNull()) {
		return true;  // No crawled_at, consider stale
	}

	// Get sitemap metadata for this URL using SURT key
	std::string surt_key = GenerateSurtKey(url);
	auto cache_result = conn.Query(
	    "SELECT lastmod, changefreq FROM _crawl_sitemap_cache WHERE surt_key = $1", surt_key);

	if (cache_result->HasError()) {
		return false;
	}

	auto cache_chunk = cache_result->Fetch();
	if (!cache_chunk || cache_chunk->size() == 0) {
		return false;  // No sitemap metadata
	}

	auto lastmod_val = cache_chunk->GetValue(0, 0);
	auto changefreq_val = cache_chunk->GetValue(1, 0);

	// If lastmod exists and is after crawled_at, URL is stale
	if (!lastmod_val.IsNull()) {
		// Compare timestamps
		auto compare_result = conn.Query(
		    "SELECT $1::TIMESTAMP > $2::TIMESTAMP",
		    lastmod_val, crawled_val);
		if (!compare_result->HasError()) {
			auto cmp_chunk = compare_result->Fetch();
			if (cmp_chunk && cmp_chunk->size() > 0) {
				auto cmp_val = cmp_chunk->GetValue(0, 0);
				if (!cmp_val.IsNull() && cmp_val.GetValue<bool>()) {
					return true;  // lastmod > crawled_at
				}
			}
		}
	}

	// Fall back to changefreq-based staleness
	std::string changefreq = changefreq_val.IsNull() ? "" : StringValue::Get(changefreq_val);
	double hours_threshold = ChangefreqToHours(changefreq);

	// Check if enough time has passed since last crawl
	auto age_result = conn.Query(
	    "SELECT EXTRACT(EPOCH FROM (NOW() - $1::TIMESTAMP)) / 3600.0 > $2",
	    crawled_val, hours_threshold);

	if (!age_result->HasError()) {
		auto age_chunk = age_result->Fetch();
		if (age_chunk && age_chunk->size() > 0) {
			auto age_val = age_chunk->GetValue(0, 0);
			if (!age_val.IsNull() && age_val.GetValue<bool>()) {
				return true;  // Age exceeds threshold
			}
		}
	}

	return false;
}

// Helper: Get cached sitemap URLs if still valid (using SURT prefix matching)
static std::vector<std::string> GetCachedSitemapUrls(Connection &conn, const std::string &hostname,
                                                      double cache_hours) {
	std::vector<std::string> urls;

	// Generate domain SURT prefix for matching (e.g., "fi,normal)" for normal.fi)
	std::string surt_prefix = GenerateDomainSurt(hostname);

	// Query using LIKE prefix match on surt_key
	auto result = conn.Query(
	    "SELECT url FROM _crawl_sitemap_cache "
	    "WHERE surt_key LIKE $1 AND discovered_at > NOW() - INTERVAL '" + std::to_string(cache_hours) + " hours'",
	    surt_prefix + "%");

	if (!result->HasError()) {
		while (auto chunk = result->Fetch()) {
			for (idx_t i = 0; i < chunk->size(); i++) {
				auto val = chunk->GetValue(0, i);
				if (!val.IsNull()) {
					urls.push_back(StringValue::Get(val));
				}
			}
		}
	}

	return urls;
}

// Helper: Cache discovered sitemap URLs with SURT keys for fast lookups
static void CacheSitemapUrls(Connection &conn, const std::string &hostname,
                              const std::vector<SitemapEntry> &entries) {
	// Generate domain SURT prefix for this hostname
	std::string surt_prefix = GenerateDomainSurt(hostname);

	// Clear old cache for this domain using LIKE prefix
	conn.Query("DELETE FROM _crawl_sitemap_cache WHERE surt_key LIKE $1", surt_prefix + "%");

	// Insert entries with SURT keys
	for (const auto &entry : entries) {
		std::string surt_key = GenerateSurtKey(entry.loc);

		// Parse lastmod - DuckDB handles ISO8601 and date-only formats
		Value lastmod_val = entry.lastmod.empty() ? Value() : Value(entry.lastmod);
		Value changefreq_val = entry.changefreq.empty() ? Value() : Value(entry.changefreq);
		Value priority_val = entry.priority.empty() ? Value() : Value(std::stod(entry.priority));

		conn.Query("INSERT OR REPLACE INTO _crawl_sitemap_cache (surt_key, url, lastmod, changefreq, priority) "
		           "VALUES ($1, $2, $3, $4, $5)",
		           surt_key, entry.loc, lastmod_val, changefreq_val, priority_val);
	}
}

//===--------------------------------------------------------------------===//
// Discovery Status Cache (tracks sitemap vs link-crawl vs not_found)
//===--------------------------------------------------------------------===//

struct DiscoveryStatus {
	bool is_valid = false;
	std::string method;  // "sitemap", "link_crawl", "not_found"
	int urls_found = 0;
};

static void EnsureDiscoveryStatusTable(Connection &conn) {
	// Check if table exists and has old schema (hostname column)
	auto check_result = conn.Query("SELECT 1 FROM _crawl_sitemap_discovery_status LIMIT 0");
	if (!check_result->HasError()) {
		auto col_check = conn.Query("SELECT hostname FROM _crawl_sitemap_discovery_status LIMIT 0");
		if (!col_check->HasError()) {
			// Old schema - drop and recreate
			conn.Query("DROP TABLE _crawl_sitemap_discovery_status");
		}
	}

	conn.Query("CREATE TABLE IF NOT EXISTS _crawl_sitemap_discovery_status ("
	           "domain_surt VARCHAR PRIMARY KEY, "  // SURT key for fast lookups
	           "discovery_method VARCHAR, "
	           "discovered_at TIMESTAMP DEFAULT NOW(), "
	           "urls_found INTEGER DEFAULT 0)");
}

static DiscoveryStatus GetDiscoveryStatus(Connection &conn, const std::string &hostname, double cache_hours) {
	DiscoveryStatus status;
	std::string domain_surt = GenerateDomainSurt(hostname);

	auto result = conn.Query(
	    "SELECT discovery_method, urls_found FROM _crawl_sitemap_discovery_status "
	    "WHERE domain_surt = $1 AND discovered_at > NOW() - INTERVAL '" + std::to_string(cache_hours) + " hours'",
	    domain_surt);

	if (!result->HasError()) {
		auto chunk = result->Fetch();
		if (chunk && chunk->size() > 0) {
			status.is_valid = true;
			auto method_val = chunk->GetValue(0, 0);
			auto count_val = chunk->GetValue(1, 0);
			status.method = method_val.IsNull() ? "" : StringValue::Get(method_val);
			status.urls_found = count_val.IsNull() ? 0 : count_val.GetValue<int>();
		}
	}
	return status;
}

static void UpdateDiscoveryStatus(Connection &conn, const std::string &hostname,
                                   const std::string &method, int urls_found) {
	std::string domain_surt = GenerateDomainSurt(hostname);
	conn.Query("INSERT OR REPLACE INTO _crawl_sitemap_discovery_status "
	           "(domain_surt, discovery_method, discovered_at, urls_found) "
	           "VALUES ($1, $2, NOW(), $3)",
	           domain_surt, method, urls_found);
}

// Result struct for batch inserts (different from CrawlResult used by crawl() function)
struct BatchCrawlEntry {
	std::string url;
	std::string surt_key;
	int status_code;
	std::string body;
	std::string content_type;
	int64_t elapsed_ms;
	std::string timestamp_expr;  // SQL expression for timestamp
	std::string error;
	std::string etag;
	std::string last_modified;
	std::string content_hash;
	std::string final_url;       // Final URL after redirects
	int redirect_count;          // Number of redirects followed
	// Structured data extraction
	std::string jsonld;          // JSON-LD data as JSON
	std::string opengraph;       // OpenGraph data as JSON
	std::string meta;            // Meta tags as JSON
	std::string hydration;       // Hydration data as JSON
	std::string js;              // JavaScript variables as JSON
	bool is_update;
	// EXTRACT clause - extracted values (parallel to extract_specs)
	std::vector<std::string> extracted_values;
};

// Flush batch of results to database
static int64_t FlushBatch(Connection &conn, const std::string &target_table,
                           std::vector<BatchCrawlEntry> &batch,
                           const std::vector<ExtractSpec> &extract_specs) {
	if (batch.empty()) {
		return 0;
	}

	int64_t rows_changed = 0;

	auto quoted_table = QuoteSqlIdentifier(target_table);

	// Check if using EXTRACT mode (dynamic schema)
	bool use_extract_mode = !extract_specs.empty();

	for (const auto &result : batch) {
		if (use_extract_mode) {
			// EXTRACT mode - dynamic columns with literal values
			std::string columns = "url, surt_key, http_status, crawled_at, error";
			std::string values_part = EscapeSqlString(result.url) + ", " +
			                          EscapeSqlString(result.surt_key) + ", " +
			                          std::to_string(result.status_code) + ", " +
			                          result.timestamp_expr + ", " +
			                          (result.error.empty() ? "NULL" : EscapeSqlString(result.error));

			for (size_t i = 0; i < extract_specs.size(); i++) {
				columns += ", " + QuoteSqlIdentifier(extract_specs[i].alias);
				if (i < result.extracted_values.size()) {
					values_part += ", " + (result.extracted_values[i].empty() ? "NULL" : EscapeSqlString(result.extracted_values[i]));
				} else {
					values_part += ", NULL";
				}
			}

			auto insert_sql = "INSERT INTO " + quoted_table + " (" + columns + ") VALUES (" + values_part + ")"
			                  " ON CONFLICT (url) DO UPDATE SET surt_key = EXCLUDED.surt_key, http_status = EXCLUDED.http_status, "
			                  "crawled_at = EXCLUDED.crawled_at, error = EXCLUDED.error";

			// Build SET clause for extracted columns
			for (const auto &spec : extract_specs) {
				insert_sql += ", " + QuoteSqlIdentifier(spec.alias) + " = EXCLUDED." + QuoteSqlIdentifier(spec.alias);
			}

			auto query_result = conn.Query(insert_sql);
			if (!query_result->HasError()) {
				rows_changed++;
			}
		} else if (result.is_update) {
			// Standard mode - update existing row
			auto update_sql = "UPDATE " + quoted_table +
			                  " SET surt_key = $2, http_status = $3, body = $4, content_type = $5, "
			                  "elapsed_ms = $6, crawled_at = " + result.timestamp_expr + ", error = $7, "
			                  "etag = $8, last_modified = $9, content_hash = $10, "
			                  "final_url = $11, redirect_count = $12, "
			                  "jsonld = $13, opengraph = $14, meta = $15, hydration = $16, js = $17 "
			                  "WHERE url = $1";

			auto update_result = conn.Query(update_sql, result.url, result.surt_key, result.status_code,
			                                 result.body.empty() ? Value() : Value(result.body),
			                                 result.content_type.empty() ? Value() : Value(result.content_type),
			                                 result.elapsed_ms,
			                                 result.error.empty() ? Value() : Value(result.error),
			                                 result.etag.empty() ? Value() : Value(result.etag),
			                                 result.last_modified.empty() ? Value() : Value(result.last_modified),
			                                 result.content_hash.empty() ? Value() : Value(result.content_hash),
			                                 result.final_url.empty() ? Value() : Value(result.final_url),
			                                 result.redirect_count,
			                                 result.jsonld.empty() ? Value("{}") : Value(result.jsonld),
			                                 result.opengraph.empty() ? Value("{}") : Value(result.opengraph),
			                                 result.meta.empty() ? Value("{}") : Value(result.meta),
			                                 result.hydration.empty() ? Value("{}") : Value(result.hydration),
			                                 result.js.empty() ? Value("{}") : Value(result.js));

			if (!update_result->HasError()) {
				rows_changed++;
			}
		} else {
			// Standard mode - insert new row
			auto insert_sql = "INSERT INTO " + quoted_table +
			                  " (url, surt_key, http_status, body, content_type, elapsed_ms, crawled_at, "
			                  "error, etag, last_modified, content_hash, final_url, redirect_count, "
			                  "jsonld, opengraph, meta, hydration, js) "
			                  "VALUES ($1, $2, $3, $4, $5, $6, " + result.timestamp_expr + ", $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17)";

			auto insert_result = conn.Query(insert_sql, result.url, result.surt_key, result.status_code,
			                                 result.body.empty() ? Value() : Value(result.body),
			                                 result.content_type.empty() ? Value() : Value(result.content_type),
			                                 result.elapsed_ms,
			                                 result.error.empty() ? Value() : Value(result.error),
			                                 result.etag.empty() ? Value() : Value(result.etag),
			                                 result.last_modified.empty() ? Value() : Value(result.last_modified),
			                                 result.content_hash.empty() ? Value() : Value(result.content_hash),
			                                 result.final_url.empty() ? Value() : Value(result.final_url),
			                                 result.redirect_count,
			                                 result.jsonld.empty() ? Value("{}") : Value(result.jsonld),
			                                 result.opengraph.empty() ? Value("{}") : Value(result.opengraph),
			                                 result.meta.empty() ? Value("{}") : Value(result.meta),
			                                 result.hydration.empty() ? Value("{}") : Value(result.hydration),
			                                 result.js.empty() ? Value("{}") : Value(result.js));

			if (!insert_result->HasError()) {
				rows_changed++;
			}
		}
	}

	batch.clear();
	return rows_changed;
}

//===--------------------------------------------------------------------===//
// Ensure target table exists for crawl results
static void EnsureTargetTable(Connection &conn, const std::string &target_table,
                               const std::vector<ExtractSpec> &extract_specs) {
	if (extract_specs.empty()) {
		// Default fixed schema (backward compatibility)
		conn.Query("CREATE TABLE IF NOT EXISTS " + QuoteSqlIdentifier(target_table) + " ("
		           "url VARCHAR PRIMARY KEY, "
		           "surt_key VARCHAR, "
		           "http_status INTEGER, "
		           "body VARCHAR, "
		           "content_type VARCHAR, "
		           "elapsed_ms BIGINT, "
		           "crawled_at TIMESTAMP, "
		           "error VARCHAR, "
		           "etag VARCHAR, "
		           "last_modified VARCHAR, "
		           "content_hash VARCHAR, "
		           "error_type VARCHAR, "
		           "final_url VARCHAR, "
		           "redirect_count INTEGER, "
		           "jsonld VARCHAR, "
		           "opengraph VARCHAR, "
		           "meta VARCHAR, "
		           "hydration VARCHAR, "
		           "js VARCHAR)");
	} else {
		// Dynamic schema based on EXTRACT clause
		std::string sql = "CREATE TABLE IF NOT EXISTS " + QuoteSqlIdentifier(target_table) + " ("
		                  "url VARCHAR PRIMARY KEY, "
		                  "surt_key VARCHAR, "
		                  "http_status INTEGER, "
		                  "crawled_at TIMESTAMP, "
		                  "error VARCHAR";

		for (const auto &spec : extract_specs) {
			sql += ", " + QuoteSqlIdentifier(spec.alias) + " VARCHAR";
		}
		sql += ")";
		conn.Query(sql);
	}
}

// Input type for CRAWL SITES
enum class SiteInputType {
	HOSTNAME,        // "example.com" - discover sitemap, fallback to spider
	HOSTNAME_PATH,   // "example.com/blog/" - discover sitemap, spider from path
	DIRECT_SITEMAP,  // "https://example.com/sitemap.xml" - fetch sitemap directly
	PAGE_URL         // "https://example.com/page?q=1" - discover sitemap, spider from URL
};

struct ParsedSiteInput {
	std::string hostname;
	std::string start_url;      // For spider fallback
	std::string sitemap_url;    // If direct sitemap provided
	SiteInputType type;
};

// Check if URL looks like a sitemap
static bool IsSitemapUrl(const std::string &url) {
	std::string lower = url;
	std::transform(lower.begin(), lower.end(), lower.begin(), ::tolower);

	// Check common sitemap patterns
	if (lower.find("/sitemap") != std::string::npos) return true;
	if (lower.length() > 4 && lower.substr(lower.length() - 4) == ".xml") return true;
	if (lower.find("sitemap_index") != std::string::npos) return true;

	return false;
}

// Helper: Check if a string looks like a valid URL or hostname using curl's URL parser
// Valid formats: "example.com", "example.com/path", "https://example.com", "http://example.com/path"
// Also supports: "localhost", "localhost:8080/path", "http://localhost:8080/path"
static bool IsValidUrlOrHostname(const std::string &input) {
	if (input.empty()) return false;

	// Build URL string for curl to parse
	std::string url_to_parse = input;

	// If no protocol, prepend https:// so curl can parse it
	if (input.find("://") == std::string::npos) {
		url_to_parse = "https://" + input;
	} else {
		// Must start with http:// or https://
		if (input.substr(0, 7) != "http://" && input.substr(0, 8) != "https://") {
			return false;
		}
	}

	// Use curl's URL parser
	CURLU *url_handle = curl_url();
	if (!url_handle) return false;

	CURLUcode result = curl_url_set(url_handle, CURLUPART_URL, url_to_parse.c_str(), 0);
	if (result != CURLUE_OK) {
		curl_url_cleanup(url_handle);
		return false;
	}

	// Get host to verify it exists
	char *host = nullptr;
	result = curl_url_get(url_handle, CURLUPART_HOST, &host, 0);
	bool valid = (result == CURLUE_OK && host != nullptr && strlen(host) > 0);

	if (host) curl_free(host);
	curl_url_cleanup(url_handle);

	return valid;
}

// Helper: Parse site input - handles various formats:
// - "example.com" → discover sitemap, spider from /
// - "example.com/blog/" → discover sitemap, spider from /blog/
// - "https://example.com/sitemap.xml" → fetch sitemap directly
// - "https://example.com/page?q=1" → discover sitemap, spider from page URL
static ParsedSiteInput ParseSiteInput(const std::string &input) {
	ParsedSiteInput result;

	// Check if it's a full URL
	if (input.find("://") != std::string::npos) {
		result.hostname = LinkParser::ExtractDomain(input);

		if (IsSitemapUrl(input)) {
			// Direct sitemap URL
			result.type = SiteInputType::DIRECT_SITEMAP;
			result.sitemap_url = input;
			result.start_url = "https://" + result.hostname + "/";
		} else {
			// Page URL - use as spider start point
			result.type = SiteInputType::PAGE_URL;
			result.start_url = input;
		}
	} else {
		// Could be "example.com" or "example.com/path/"
		size_t slash_pos = input.find('/');
		if (slash_pos != std::string::npos) {
			result.hostname = input.substr(0, slash_pos);
			std::string path = input.substr(slash_pos);
			result.start_url = "https://" + result.hostname + path;
			result.type = SiteInputType::HOSTNAME_PATH;
		} else {
			result.hostname = input;
			result.start_url = "https://" + result.hostname + "/";
			result.type = SiteInputType::HOSTNAME;
		}
	}

	return result;
}

//===--------------------------------------------------------------------===//
// Link-Based URL Discovery (BFS Crawling)
//===--------------------------------------------------------------------===//

// Discover URLs by following links (used as fallback when no sitemap)
static std::vector<std::string> DiscoverUrlsViaLinks(
    DatabaseInstance &db,
    const std::string &start_url,
    const std::string &base_hostname,
    const std::string &user_agent,
    double default_crawl_delay,
    bool allow_subdomains,
    int max_pages,
    int max_depth,
    bool respect_nofollow,
    bool follow_canonical,
    DomainState &domain_state) {

	std::vector<std::string> discovered_urls;
	std::set<std::string> visited_urls;
	std::set<std::string> queued_urls;

	// BFS queue: (url, depth)
	std::queue<std::pair<std::string, int>> url_queue;
	url_queue.push({start_url, 0});
	queued_urls.insert(start_url);

	std::string base_domain = LinkParser::ExtractBaseDomain(base_hostname);

	RetryConfig retry_config;
	retry_config.max_retries = 2;

	int pages_crawled = 0;

	while (!url_queue.empty() && !g_shutdown_requested && pages_crawled < max_pages) {
		auto front = url_queue.front();
		url_queue.pop();
		const std::string &url = front.first;
		int depth = front.second;

		if (depth > max_depth) {
			continue;
		}

		if (visited_urls.count(url)) {
			continue;
		}
		visited_urls.insert(url);

		std::string url_domain = LinkParser::ExtractDomain(url);
		std::string url_path = LinkParser::ExtractPath(url);

		// Check robots.txt (only for base domain, assume fetched already)
		if (url_domain == base_hostname && domain_state.robots_fetched) {
			if (!RobotsParser::IsAllowed(domain_state.rules, url_path)) {
				continue;  // Disallowed by robots.txt
			}
		}

		// Respect rate limiting
		auto now = std::chrono::steady_clock::now();
		auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(
		    now - domain_state.last_crawl_time).count();
		auto required_delay_ms = static_cast<int64_t>(domain_state.crawl_delay_seconds * 1000);

		if (elapsed < required_delay_ms) {
			std::this_thread::sleep_for(std::chrono::milliseconds(required_delay_ms - elapsed));
		}

		// Fetch page
		auto response = HttpClient::Fetch(url, retry_config, user_agent);
		domain_state.last_crawl_time = std::chrono::steady_clock::now();

		if (!response.success || response.status_code != 200) {
			continue;
		}

		// Check if HTML
		if (response.content_type.find("text/html") == std::string::npos) {
			discovered_urls.push_back(url);
			pages_crawled++;
			continue;
		}

		// Check for nofollow meta tag
		if (respect_nofollow && LinkParser::HasNoFollowMeta(response.body)) {
			discovered_urls.push_back(url);
			pages_crawled++;
			continue;  // Don't follow links from this page
		}

		// Check for canonical URL
		if (follow_canonical) {
			std::string canonical = LinkParser::ExtractCanonical(response.body, url);
			if (!canonical.empty() && canonical != url) {
				// Use canonical URL instead
				if (!visited_urls.count(canonical) && !queued_urls.count(canonical)) {
					if (LinkParser::IsSameDomain(canonical, base_domain, allow_subdomains)) {
						queued_urls.insert(canonical);
						url_queue.push({canonical, depth});
					}
				}
				continue;  // Skip this URL, use canonical
			}
		}

		discovered_urls.push_back(url);
		pages_crawled++;

		// Extract links from page
		auto links = LinkParser::ExtractLinks(response.body, url);

		for (const auto &link : links) {
			// Skip nofollow links if respect_nofollow is enabled
			if (respect_nofollow && link.nofollow) {
				continue;
			}

			// Skip already visited or queued
			if (visited_urls.count(link.url) || queued_urls.count(link.url)) {
				continue;
			}

			// Check domain policy
			if (!LinkParser::IsSameDomain(link.url, base_domain, allow_subdomains)) {
				continue;
			}

			// Only follow http/https
			if (link.url.find("https://") != 0 && link.url.find("http://") != 0) {
				continue;
			}

			queued_urls.insert(link.url);
			url_queue.push({link.url, depth + 1});
		}
	}

	return discovered_urls;
}

// Result from parallel sitemap discovery
struct SitemapDiscoveryResult {
	std::string hostname;
	std::string start_url;  // Full URL including path
	std::vector<std::string> urls;
	DomainState domain_state;
};

// Thread-safe sitemap discovery for parallel execution (with link fallback support)
// Handles various input formats:
// - "example.com" → robots.txt → bruteforce sitemaps → spider (if follow_links)
// - "example.com/path/" → robots.txt → bruteforce sitemaps → spider from path
// - "https://example.com/sitemap.xml" → fetch sitemap directly (still check robots.txt)
// - "https://example.com/page?q=1" → robots.txt → sitemaps → spider from page URL
static SitemapDiscoveryResult DiscoverSitemapUrlsThreadSafe(DatabaseInstance &db,
                                                             const std::string &input,
                                                             const std::string &user_agent,
                                                             double default_crawl_delay, double cache_hours,
                                                             bool follow_links, bool allow_subdomains,
                                                             int max_crawl_pages, int max_crawl_depth,
                                                             bool respect_nofollow, bool follow_canonical) {
	// Parse input to determine type and extract hostname/URLs
	auto parsed = ParseSiteInput(input);

	SitemapDiscoveryResult result;
	result.hostname = parsed.hostname;
	result.start_url = parsed.start_url;
	result.domain_state.crawl_delay_seconds = default_crawl_delay;
	result.domain_state.min_crawl_delay_seconds = default_crawl_delay;

	// Create own connection for thread safety
	Connection conn(db);

	// Check discovery status cache first (skip for direct sitemap URLs)
	if (parsed.type != SiteInputType::DIRECT_SITEMAP) {
		EnsureDiscoveryStatusTable(conn);
		auto status = GetDiscoveryStatus(conn, parsed.hostname, cache_hours);

		if (status.is_valid) {
			if (status.method == "sitemap" || status.method == "link_crawl") {
				// Check sitemap URL cache
				EnsureSitemapCacheTable(conn);
				auto cached_urls = GetCachedSitemapUrls(conn, parsed.hostname, cache_hours);
				if (!cached_urls.empty()) {
					result.urls = cached_urls;
					return result;
				}
			} else if (status.method == "not_found" && !follow_links) {
				// Previously found nothing and no fallback enabled
				return result;
			}
			// If not_found but follow_links now enabled, retry
		}
	}

	// Ensure cache table exists
	EnsureSitemapCacheTable(conn);

	// Try sitemap discovery
	std::vector<SitemapEntry> sitemap_entries;
	std::set<std::string> processed_sitemaps;
	std::vector<std::string> to_process;

	RetryConfig retry_config;
	retry_config.max_retries = 2;

	// Step 1: Always fetch robots.txt for rate limiting (even for direct sitemap URLs)
	std::string robots_url = "https://" + parsed.hostname + "/robots.txt";
	auto robots_response = HttpClient::Fetch(robots_url, retry_config, user_agent);

	if (robots_response.success) {
		// Parse robots rules for rate limiting
		auto robots_data = RobotsParser::Parse(robots_response.body);
		result.domain_state.rules = RobotsParser::GetRulesForUserAgent(robots_data, user_agent);
		result.domain_state.has_crawl_delay = result.domain_state.rules.HasCrawlDelay();
		if (result.domain_state.has_crawl_delay) {
			result.domain_state.crawl_delay_seconds = result.domain_state.rules.GetEffectiveDelay();
		}
		result.domain_state.min_crawl_delay_seconds = result.domain_state.crawl_delay_seconds;
		result.domain_state.robots_fetched = true;

		// Extract sitemaps from robots.txt (unless we have a direct sitemap URL)
		if (parsed.type != SiteInputType::DIRECT_SITEMAP) {
			auto sitemaps_from_robots = SitemapParser::ExtractSitemapsFromRobotsTxt(robots_response.body);
			for (auto &sm : sitemaps_from_robots) {
				to_process.push_back(sm);
			}
		}
	}

	// Step 2: Handle based on input type
	if (parsed.type == SiteInputType::DIRECT_SITEMAP) {
		// Direct sitemap URL provided - use it directly
		to_process.push_back(parsed.sitemap_url);
	} else if (to_process.empty()) {
		// No sitemaps from robots.txt - bruteforce common locations
		auto common_paths = SitemapParser::GetCommonSitemapPaths();
		for (auto &path : common_paths) {
			to_process.push_back("https://" + parsed.hostname + path);
		}
	}

	// Step 3: Fetch and parse sitemaps (handle sitemap indexes recursively)
	while (!to_process.empty() && !g_shutdown_requested) {
		std::string sitemap_url = to_process.back();
		to_process.pop_back();

		if (processed_sitemaps.count(sitemap_url)) {
			continue;
		}
		processed_sitemaps.insert(sitemap_url);

		auto response = HttpClient::Fetch(sitemap_url, retry_config, user_agent);
		if (!response.success || response.status_code != 200) {
			continue;
		}

		// Decompress gzipped sitemaps (check magic bytes, not URL extension)
		// Note: DuckDB's http_get may auto-decompress, so we check actual data
		std::string body = response.body;
		if (IsGzippedData(body)) {
			body = DecompressGzip(response.body);
			if (body.empty()) {
				continue;  // Decompression failed
			}
		}

		auto sitemap_data = SitemapParser::Parse(body);

		if (sitemap_data.is_index) {
			for (auto &nested_url : sitemap_data.sitemap_urls) {
				if (!processed_sitemaps.count(nested_url)) {
					to_process.push_back(nested_url);
				}
			}
		} else {
			for (auto &entry : sitemap_data.urls) {
				sitemap_entries.push_back(entry);
			}
		}
	}

	// If sitemap found, cache and return
	if (!sitemap_entries.empty()) {
		CacheSitemapUrls(conn, parsed.hostname, sitemap_entries);
		UpdateDiscoveryStatus(conn, parsed.hostname, "sitemap", sitemap_entries.size());

		for (const auto &entry : sitemap_entries) {
			result.urls.push_back(entry.loc);
		}
		return result;
	}

	// No sitemap found - try link-based discovery if enabled
	// For PAGE_URL type, always try spider from the provided URL (if follow_links)
	// For HOSTNAME/HOSTNAME_PATH, spider from start_url
	if (follow_links) {
		auto link_urls = DiscoverUrlsViaLinks(
		    db, parsed.start_url, parsed.hostname, user_agent, default_crawl_delay,
		    allow_subdomains, max_crawl_pages, max_crawl_depth,
		    respect_nofollow, follow_canonical, result.domain_state);

		if (!link_urls.empty()) {
			// Cache discovered URLs
			std::vector<SitemapEntry> entries;
			for (const auto &url : link_urls) {
				entries.push_back({url, "", "", ""});
			}
			CacheSitemapUrls(conn, parsed.hostname, entries);
			UpdateDiscoveryStatus(conn, parsed.hostname, "link_crawl", link_urls.size());

			result.urls = link_urls;
			return result;
		}
	}

	// For PAGE_URL type: if no sitemap found, at least include the original URL for crawling
	// This ensures a simple "CRAWL (SELECT 'https://example.com/')" works
	if (parsed.type == SiteInputType::PAGE_URL && result.urls.empty()) {
		result.urls.push_back(parsed.start_url);
		// Don't cache "not_found" since we're returning the original URL
		return result;
	}

	// Nothing found - cache negative result (but not for direct sitemap - those should just return empty)
	if (parsed.type != SiteInputType::DIRECT_SITEMAP) {
		UpdateDiscoveryStatus(conn, parsed.hostname, "not_found", 0);
	}
	return result;
}

// Helper: Discover sitemaps for a hostname (with caching)
static std::vector<std::string> DiscoverSitemapUrls(ClientContext &context, Connection &conn,
                                                     const std::string &hostname,
                                                     const std::string &user_agent,
                                                     std::unordered_map<std::string, DomainState> &domain_states,
                                                     double default_crawl_delay, double cache_hours) {
	// Check cache first
	EnsureSitemapCacheTable(conn);
	auto cached_urls = GetCachedSitemapUrls(conn, hostname, cache_hours);
	if (!cached_urls.empty()) {
		return cached_urls;
	}

	// No valid cache - discover sitemaps
	std::vector<SitemapEntry> sitemap_entries;  // Keep full metadata
	std::set<std::string> processed_sitemaps;
	std::vector<std::string> to_process;

	RetryConfig retry_config;
	retry_config.max_retries = 2;

	// Step 1: Try robots.txt for Sitemap directives
	std::string robots_url = "https://" + hostname + "/robots.txt";
	auto robots_response = HttpClient::Fetch(robots_url, retry_config, user_agent);

	if (robots_response.success) {
		auto sitemaps_from_robots = SitemapParser::ExtractSitemapsFromRobotsTxt(robots_response.body);
		for (auto &sm : sitemaps_from_robots) {
			to_process.push_back(sm);
		}

		// Also parse robots rules for rate limiting
		auto &domain_state = domain_states[hostname];
		auto robots_data = RobotsParser::Parse(robots_response.body);
		domain_state.rules = RobotsParser::GetRulesForUserAgent(robots_data, user_agent);
		domain_state.has_crawl_delay = domain_state.rules.HasCrawlDelay();
		if (domain_state.has_crawl_delay) {
			domain_state.crawl_delay_seconds = domain_state.rules.GetEffectiveDelay();
		} else {
			domain_state.crawl_delay_seconds = default_crawl_delay;
		}
		// Set floor for adaptive rate limiting
		domain_state.min_crawl_delay_seconds = domain_state.crawl_delay_seconds;
		domain_state.robots_fetched = true;
	}

	// Step 2: If no sitemaps found, bruteforce common locations
	if (to_process.empty()) {
		auto common_paths = SitemapParser::GetCommonSitemapPaths();
		for (auto &path : common_paths) {
			to_process.push_back("https://" + hostname + path);
		}
	}

	// Step 3: Fetch and parse sitemaps (handle sitemap indexes recursively)
	while (!to_process.empty() && !g_shutdown_requested) {
		std::string sitemap_url = to_process.back();
		to_process.pop_back();

		if (processed_sitemaps.count(sitemap_url)) {
			continue;
		}
		processed_sitemaps.insert(sitemap_url);

		auto response = HttpClient::Fetch(sitemap_url, retry_config, user_agent);
		if (!response.success || response.status_code != 200) {
			continue;
		}

		// Decompress gzipped sitemaps (check magic bytes, not URL extension)
		std::string body = response.body;
		if (IsGzippedData(body)) {
			body = DecompressGzip(response.body);
			if (body.empty()) {
				continue;  // Decompression failed
			}
		}

		auto sitemap_data = SitemapParser::Parse(body);

		if (sitemap_data.is_index) {
			// Sitemap index - add nested sitemaps to process
			for (auto &nested_url : sitemap_data.sitemap_urls) {
				if (!processed_sitemaps.count(nested_url)) {
					to_process.push_back(nested_url);
				}
			}
		} else {
			// Regular sitemap - keep full entry with metadata
			for (auto &entry : sitemap_data.urls) {
				sitemap_entries.push_back(entry);
			}
		}
	}

	// Cache discovered entries with metadata for future runs
	if (!sitemap_entries.empty()) {
		CacheSitemapUrls(conn, hostname, sitemap_entries);
	}

	// Return just URLs for compatibility
	std::vector<std::string> urls;
	for (const auto &entry : sitemap_entries) {
		urls.push_back(entry.loc);
	}
	return urls;
}

//===--------------------------------------------------------------------===//
// Multi-Threaded Crawl Worker
//===--------------------------------------------------------------------===//

// Worker function for parallel crawling
// Processes URLs from queue, respects rate limits, and batches results
static void CrawlWorker(
    int worker_id,
    ThreadSafeUrlQueue &url_queue,
    ThreadSafeDomainMap &domain_states,
    const CrawlIntoBindData &bind_data,
    std::vector<BatchCrawlEntry> &result_batch,
    std::mutex &batch_mutex,
    std::mutex &db_mutex,
    Connection &conn,
    std::atomic<int64_t> &rows_changed,
    std::atomic<int64_t> &processed_urls,
    std::atomic<bool> &should_stop,
    std::atomic<int> &in_flight
) {
	const size_t LOCAL_BATCH_SIZE = 20;
	std::vector<BatchCrawlEntry> local_batch;
	local_batch.reserve(LOCAL_BATCH_SIZE);

	while (!should_stop.load()) {
		UrlQueueEntry entry("", 0, false);
		if (!url_queue.WaitAndPop(entry, std::chrono::milliseconds(100))) {
			if (url_queue.Empty() && in_flight.load() == 0) {
				break;  // Queue exhausted and no one processing, exit worker
			}
			continue;  // Timeout, try again
		}

		// Track in-flight requests for shutdown coordination
		in_flight.fetch_add(1);

		// Note: We don't check should_stop here because we already popped the entry.
		// We should process it before exiting to avoid losing work.

		std::string domain = ExtractDomain(entry.url);
		std::string path = ExtractPath(entry.url);
		auto &domain_state = domain_states.GetOrCreate(domain);

		// Check if domain is blocked (429 backoff)
		{
			std::lock_guard<std::mutex> lock(domain_state.mutex);
			auto now = std::chrono::steady_clock::now();
			if (domain_state.blocked_until > now) {
				// Re-queue with later time
				entry.earliest_fetch = domain_state.blocked_until;
				if (entry.retry_count < 5) {
					entry.retry_count++;
					url_queue.Push(std::move(entry));
				}
				in_flight.fetch_sub(1);
				continue;
			}
		}

		// Fetch robots.txt if needed
		bool robots_allow = true;
		{
			std::lock_guard<std::mutex> lock(domain_state.mutex);
			if (bind_data.respect_robots_txt && !domain_state.robots_fetched) {
				// Release lock while fetching
			}
		}

		// Fetch robots.txt outside lock if needed
		if (bind_data.respect_robots_txt) {
			bool need_fetch = false;
			{
				std::lock_guard<std::mutex> lock(domain_state.mutex);
				need_fetch = !domain_state.robots_fetched;
			}

			if (need_fetch) {
				std::string robots_url = "https://" + domain + "/robots.txt";
				RetryConfig retry_config;
				auto response = HttpClient::Fetch(robots_url, retry_config, bind_data.user_agent);

				std::lock_guard<std::mutex> lock(domain_state.mutex);
				if (!domain_state.robots_fetched) {  // Double-check after acquiring lock
					if (response.success) {
						auto robots_data = RobotsParser::Parse(response.body);
						domain_state.rules = RobotsParser::GetRulesForUserAgent(robots_data, bind_data.user_agent);
						domain_state.has_crawl_delay = domain_state.rules.HasCrawlDelay();
						if (domain_state.has_crawl_delay) {
							domain_state.crawl_delay_seconds = domain_state.rules.GetEffectiveDelay();
						} else {
							domain_state.crawl_delay_seconds = bind_data.default_crawl_delay;
						}
						domain_state.crawl_delay_seconds = std::max(domain_state.crawl_delay_seconds, bind_data.min_crawl_delay);
						domain_state.crawl_delay_seconds = std::min(domain_state.crawl_delay_seconds, bind_data.max_crawl_delay);
						domain_state.min_crawl_delay_seconds = domain_state.crawl_delay_seconds;
					} else {
						domain_state.crawl_delay_seconds = bind_data.default_crawl_delay;
						domain_state.min_crawl_delay_seconds = bind_data.default_crawl_delay;
						domain_state.has_crawl_delay = false;
					}
					domain_state.robots_fetched = true;
				}
			}
		}

		// Check robots.txt rules
		{
			std::lock_guard<std::mutex> lock(domain_state.mutex);
			if (bind_data.respect_robots_txt && !RobotsParser::IsAllowed(domain_state.rules, path)) {
				robots_allow = false;
			}
		}

		if (!robots_allow) {
			if (bind_data.log_skipped) {
				std::string surt_key = GenerateSurtKey(entry.url);
				local_batch.push_back(BatchCrawlEntry{
					entry.url, surt_key, -1, "", "", 0, "NOW()",
					"robots.txt disallow", "", "", "", "", 0,
					"", "", "", "", "", entry.is_update
				});
			}
			processed_urls.fetch_add(1);
			in_flight.fetch_sub(1);
			continue;
		}

		// Enforce per-domain rate limit
		// IMPORTANT: Update last_crawl_time while holding lock to prevent race condition
		// where multiple threads see "sufficient delay" and all proceed simultaneously
		{
			std::lock_guard<std::mutex> lock(domain_state.mutex);
			auto now = std::chrono::steady_clock::now();
			auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(
			    now - domain_state.last_crawl_time).count();
			auto required_delay_ms = static_cast<int64_t>(domain_state.crawl_delay_seconds * 1000);

			if (elapsed < required_delay_ms) {
				// Re-queue with future timestamp
				entry.earliest_fetch = domain_state.last_crawl_time +
				    std::chrono::milliseconds(required_delay_ms);
				url_queue.Push(std::move(entry));
				in_flight.fetch_sub(1);
				continue;
			}

			// Reserve this slot by updating last_crawl_time NOW
			// This prevents other threads from also seeing "sufficient delay"
			domain_state.last_crawl_time = now;
		}

		// Wait for global connection limit (but don't check should_stop - we have the entry, process it)
		while (g_active_connections >= bind_data.max_total_connections) {
			std::this_thread::sleep_for(std::chrono::milliseconds(10));
		}

		// Fetch URL
		g_active_connections++;
		auto fetch_start = std::chrono::steady_clock::now();
		RetryConfig retry_config;
		auto response = HttpClient::Fetch(entry.url, retry_config, bind_data.user_agent, bind_data.compress);
		auto fetch_end = std::chrono::steady_clock::now();
		auto fetch_elapsed_ms = std::chrono::duration_cast<std::chrono::milliseconds>(fetch_end - fetch_start).count();
		g_active_connections--;

		// Update domain state
		{
			std::lock_guard<std::mutex> lock(domain_state.mutex);
			domain_state.last_crawl_time = fetch_end;
		}

		// Check response size limit
		if (response.success && static_cast<int64_t>(response.body.size()) > bind_data.max_response_bytes) {
			response.success = false;
			response.error = "Response too large: " + std::to_string(response.body.size()) + " bytes";
			response.body.clear();
		}

		// Check content-type filter
		if (response.success && !IsContentTypeAcceptable(response.content_type,
		                                                   bind_data.accept_content_types,
		                                                   bind_data.reject_content_types)) {
			response.success = false;
			response.error = "Content-Type rejected: " + response.content_type;
			response.body.clear();
		}

		// Check for meta robots noindex
		if (response.success && bind_data.respect_robots_txt &&
		    response.content_type.find("text/html") != std::string::npos &&
		    LinkParser::HasNoIndexMeta(response.body)) {
			response.body.clear();
		}

		// Check for retryable errors (429, 5XX, network errors)
		bool is_retryable = (response.status_code == 429 ||
		                     (response.status_code >= 500 && response.status_code <= 504) ||
		                     response.status_code <= 0);

		if (is_retryable) {
			std::lock_guard<std::mutex> lock(domain_state.mutex);
			domain_state.consecutive_429s++;

			int backoff_seconds;
			if (response.status_code == 429 && !response.retry_after.empty()) {
				backoff_seconds = HttpClient::ParseRetryAfter(response.retry_after) / 1000;
				if (backoff_seconds <= 0) {
					backoff_seconds = FibonacciBackoffSeconds(domain_state.consecutive_429s,
					                                           bind_data.max_retry_backoff_seconds);
				}
			} else {
				backoff_seconds = FibonacciBackoffSeconds(domain_state.consecutive_429s,
				                                           bind_data.max_retry_backoff_seconds);
			}

			domain_state.blocked_until = std::chrono::steady_clock::now() +
			                              std::chrono::seconds(backoff_seconds);

			if (entry.retry_count < 5) {
				entry.retry_count++;
				entry.earliest_fetch = domain_state.blocked_until;
				url_queue.Push(std::move(entry));
			}
			in_flight.fetch_sub(1);
			continue;
		}

		// Success - clear consecutive error count, unblock domain, and update adaptive delay
		{
			std::lock_guard<std::mutex> lock(domain_state.mutex);
			domain_state.consecutive_429s = 0;
			// Clear blocked_until on success - domain is responding normally again
			domain_state.blocked_until = std::chrono::steady_clock::time_point{};
			UpdateAdaptiveDelay(domain_state, static_cast<double>(fetch_elapsed_ms), bind_data.max_crawl_delay);
		}

		// Get timestamp
		std::string validated_date = ParseAndValidateServerDate(response.server_date);
		std::string timestamp_expr = validated_date.empty() ? "NOW()" : ("'" + validated_date + "'::TIMESTAMP");

		// Generate SURT key and content hash
		std::string surt_key = GenerateSurtKey(entry.url);
		std::string content_hash = GenerateContentHash(response.body);

		// Extract structured data from HTML content
		std::string jsonld_json, opengraph_json, meta_json, hydration_json, js_json;
		if (response.success && !response.body.empty() &&
		    response.content_type.find("text/html") != std::string::npos) {
			ExtractionConfig config;
			config.extract_js = bind_data.extract_js;
			auto structured = ExtractStructuredData(response.body, config);
			jsonld_json = structured.jsonld;
			opengraph_json = structured.opengraph;
			meta_json = structured.meta;
			hydration_json = structured.hydration;
			js_json = structured.js;
		}

		// Evaluate EXTRACT expressions if present
		std::vector<std::string> extracted_values;
		if (!bind_data.extract_paths.empty()) {
			for (size_t i = 0; i < bind_data.extract_paths.size(); i++) {
				const auto &path = bind_data.extract_paths[i];
				std::string source_json;

				// Get source JSON based on base column
				if (path.base_column == "jsonld") {
					source_json = jsonld_json;
				} else if (path.base_column == "opengraph") {
					source_json = opengraph_json;
				} else if (path.base_column == "meta") {
					source_json = meta_json;
				} else if (path.base_column == "hydration") {
					source_json = hydration_json;
				} else if (path.base_column == "js") {
					source_json = js_json;
				}

				std::string value = EvaluateJsonPath(source_json, path);
				extracted_values.push_back(value);
			}
		}

		// Add to local batch
		BatchCrawlEntry batch_entry;
		batch_entry.url = entry.url;
		batch_entry.surt_key = surt_key;
		batch_entry.status_code = response.status_code;
		batch_entry.body = response.body;
		batch_entry.content_type = response.content_type;
		batch_entry.elapsed_ms = fetch_elapsed_ms;
		batch_entry.timestamp_expr = timestamp_expr;
		batch_entry.error = response.error;
		batch_entry.etag = response.etag;
		batch_entry.last_modified = response.last_modified;
		batch_entry.content_hash = content_hash;
		batch_entry.final_url = response.final_url;
		batch_entry.redirect_count = response.redirect_count;
		batch_entry.jsonld = jsonld_json;
		batch_entry.opengraph = opengraph_json;
		batch_entry.meta = meta_json;
		batch_entry.hydration = hydration_json;
		batch_entry.js = js_json;
		batch_entry.is_update = entry.is_update;
		batch_entry.extracted_values = extracted_values;
		local_batch.push_back(std::move(batch_entry));

		processed_urls.fetch_add(1);

		// Flush local batch to shared batch
		if (local_batch.size() >= LOCAL_BATCH_SIZE) {
			std::lock_guard<std::mutex> lock(batch_mutex);
			for (auto &e : local_batch) {
				result_batch.push_back(std::move(e));
			}
			local_batch.clear();

			// Flush shared batch if large enough
			if (result_batch.size() >= 100) {
				std::lock_guard<std::mutex> db_lock(db_mutex);
				rows_changed.fetch_add(FlushBatch(conn, bind_data.target_table, result_batch, bind_data.extract_specs));
			}
		}

		in_flight.fetch_sub(1);
	}

	// Flush remaining local batch
	if (!local_batch.empty()) {
		std::lock_guard<std::mutex> lock(batch_mutex);
		for (auto &e : local_batch) {
			result_batch.push_back(std::move(e));
		}
	}
}

static void CrawlIntoFunction(ClientContext &context, TableFunctionInput &data, DataChunk &output) {
	auto &bind_data = data.bind_data->CastNoConst<CrawlIntoBindData>();
	auto &global_state = data.global_state->Cast<CrawlIntoGlobalState>();

	std::lock_guard<std::mutex> lock(global_state.mutex);

	if (global_state.executed) {
		output.SetCardinality(0);
		return;
	}
	global_state.executed = true;

	Connection conn(*context.db);

	// Load HTTP settings from DuckDB configuration (timeout, proxy, etc.)
	LoadHttpSettingsFromDuckDB(conn);

	// Execute source query to get URLs or hostnames
	auto query_result = conn.Query(bind_data.source_query);
	if (query_result->HasError()) {
		throw IOException("CRAWL source query error: " + query_result->GetError());
	}

	// Collect values from first column - must be valid URLs or hostnames
	std::vector<std::string> source_values;
	std::vector<std::string> invalid_values;
	while (auto chunk = query_result->Fetch()) {
		for (idx_t i = 0; i < chunk->size(); i++) {
			auto val = chunk->GetValue(0, i);
			if (!val.IsNull()) {
				// Check if value is a string type
				if (val.type().id() != LogicalTypeId::VARCHAR) {
					// Try to convert to string representation
					std::string str_val = val.ToString();
					if (!IsValidUrlOrHostname(str_val)) {
						invalid_values.push_back(str_val);
						continue;
					}
					source_values.push_back(str_val);
				} else {
					std::string str_val = StringValue::Get(val);
					if (!IsValidUrlOrHostname(str_val)) {
						invalid_values.push_back(str_val);
						continue;
					}
					source_values.push_back(str_val);
				}
			}
		}
	}

	// Report invalid values
	if (!invalid_values.empty()) {
		std::string msg = "CRAWL source query returned invalid URL/hostname values: ";
		for (size_t i = 0; i < std::min(invalid_values.size(), (size_t)5); i++) {
			if (i > 0) msg += ", ";
			msg += "'" + invalid_values[i] + "'";
		}
		if (invalid_values.size() > 5) {
			msg += " (and " + std::to_string(invalid_values.size() - 5) + " more)";
		}
		msg += ". Expected format: 'example.com', 'example.com/path', or 'https://example.com/path'";
		throw IOException(msg);
	}

	if (source_values.empty()) {
		output.SetCardinality(1);
		output.SetValue(0, 0, Value::BIGINT(0));
		return;
	}

	// Prepare domain states for rate limiting
	std::unordered_map<std::string, DomainState> domain_states;

	// Ensure target table exists for results
	EnsureTargetTable(conn, bind_data.target_table, bind_data.extract_specs);

	auto crawl_start_time = std::chrono::steady_clock::now();

	// Mark discovery as started for progress bar
	global_state.discovery_started.store(true);

	// Discover URLs from hostnames/sites - always use discovery mode
	std::vector<std::string> urls;
	auto &db = DatabaseInstance::GetDatabase(context);

	// Limit parallel discovery to max_parallel_per_domain (or 8 if not set)
	int max_parallel = std::min((int)source_values.size(), bind_data.max_parallel_per_domain);
	if (max_parallel < 1) max_parallel = 8;

	std::vector<std::future<SitemapDiscoveryResult>> futures;
	std::vector<SitemapDiscoveryResult> results;

	size_t idx = 0;
	while (idx < source_values.size() && !g_shutdown_requested) {
		// Launch batch of async tasks
		futures.clear();
		for (int i = 0; i < max_parallel && idx < source_values.size(); i++, idx++) {
			const auto &input = source_values[idx];
			futures.push_back(std::async(std::launch::async,
			    DiscoverSitemapUrlsThreadSafe,
			    std::ref(db), input, bind_data.user_agent,
			    bind_data.default_crawl_delay, bind_data.sitemap_cache_hours,
			    bind_data.follow_links, bind_data.allow_subdomains,
			    bind_data.max_crawl_pages, bind_data.max_crawl_depth,
			    bind_data.respect_nofollow, bind_data.follow_canonical));
		}

		// Collect results from this batch
		for (auto &f : futures) {
			if (g_shutdown_requested) break;
			try {
				results.push_back(f.get());
			} catch (...) {
				// Silently skip failed discoveries
			}
		}
	}

	// Merge results into domain_states and urls
	for (auto &res : results) {
		domain_states[res.hostname] = res.domain_state;
		for (const auto &url : res.urls) {
			if (bind_data.url_filter.empty() || MatchesLikePattern(url, bind_data.url_filter)) {
				urls.push_back(url);
			}
		}
	}

	if (urls.empty()) {
		output.SetCardinality(1);
		output.SetValue(0, 0, Value::BIGINT(0));
		return;
	}

	// Get existing URLs from target table
	std::set<std::string> existing_urls;
	auto existing_query = conn.Query("SELECT DISTINCT url FROM " + QuoteSqlIdentifier(bind_data.target_table));
	if (!existing_query->HasError()) {
		while (auto chunk = existing_query->Fetch()) {
			for (idx_t i = 0; i < chunk->size(); i++) {
				auto val = chunk->GetValue(0, i);
				if (!val.IsNull()) {
					existing_urls.insert(StringValue::Get(val));
				}
			}
		}
	}

	// Determine number of threads - use DuckDB's thread setting if not specified
	int num_threads = bind_data.num_threads;
	if (num_threads <= 0) {
		auto thread_result = conn.Query("SELECT current_setting('threads')::INT");
		if (!thread_result->HasError() && thread_result->RowCount() > 0) {
			auto chunk = thread_result->Fetch();
			if (chunk && chunk->size() > 0) {
				num_threads = chunk->GetValue(0, 0).GetValue<int>();
			}
		}
	}
	num_threads = std::max(1, std::min(num_threads, 32));  // Clamp to 1-32

	// Build thread-safe URL queue
	ThreadSafeUrlQueue url_queue;
	auto now = std::chrono::steady_clock::now();

	int64_t urls_added = 0;
	for (const auto &url : urls) {
		if (existing_urls.find(url) == existing_urls.end()) {
			url_queue.Push(UrlQueueEntry(url, 0, false, now));
			urls_added++;
		}
	}

	// If update_stale enabled, check existing URLs for staleness and add them
	if (bind_data.update_stale && !existing_urls.empty()) {
		EnsureSitemapCacheTable(conn);
		for (const auto &url : urls) {
			if (existing_urls.find(url) != existing_urls.end()) {
				if (IsUrlStale(conn, url, bind_data.target_table)) {
					url_queue.Push(UrlQueueEntry(url, 0, true, now));
					urls_added++;
				}
			}
		}
	}

	if (urls_added == 0) {
		output.SetCardinality(1);
		output.SetValue(0, 0, Value::BIGINT(0));
		return;
	}

	// Set total for progress bar
	global_state.total_urls.store(urls_added);
	global_state.processed_urls.store(0);

	// Create thread-safe domain map and initialize from discovery results
	ThreadSafeDomainMap thread_safe_domain_states;
	for (const auto &kv : domain_states) {
		thread_safe_domain_states.InitializeFromDiscovery(kv.first, kv.second);
	}

	// Shared state for workers
	std::vector<BatchCrawlEntry> result_batch;
	result_batch.reserve(100);
	std::mutex batch_mutex;
	std::mutex db_mutex;
	std::atomic<int64_t> rows_changed{0};
	std::atomic<bool> should_stop{false};
	std::atomic<int> in_flight{0};  // Number of URLs currently being processed

	// Check shutdown lambda
	auto check_stop = [&]() {
		return g_shutdown_requested.load();
	};

	// Launch worker threads
	std::vector<std::thread> workers;
	workers.reserve(num_threads);
	for (int i = 0; i < num_threads; i++) {
		workers.emplace_back(CrawlWorker,
		    i, std::ref(url_queue), std::ref(thread_safe_domain_states), std::ref(bind_data),
		    std::ref(result_batch), std::ref(batch_mutex), std::ref(db_mutex), std::ref(conn),
		    std::ref(rows_changed), std::ref(global_state.processed_urls), std::ref(should_stop),
		    std::ref(in_flight));
	}

	// Monitor for shutdown while workers run
	// Exit when queue is empty AND no in-flight requests (all workers idle)
	while (true) {
		if (check_stop()) {
			should_stop.store(true);
			url_queue.Shutdown();
			break;
		}

		// Check if all work is done: queue empty AND no in-flight requests
		if (url_queue.Empty() && in_flight.load() == 0) {
			// Double check after brief wait to avoid race
			std::this_thread::sleep_for(std::chrono::milliseconds(50));
			if (url_queue.Empty() && in_flight.load() == 0) {
				should_stop.store(true);
				url_queue.Shutdown();
				break;
			}
		}
		std::this_thread::sleep_for(std::chrono::milliseconds(50));
	}

	// Signal shutdown and wait for workers
	should_stop.store(true);
	url_queue.Shutdown();
	for (auto &t : workers) {
		if (t.joinable()) {
			t.join();
		}
	}

	// Final flush of any remaining results
	{
		std::lock_guard<std::mutex> lock(batch_mutex);
		if (!result_batch.empty()) {
			std::lock_guard<std::mutex> db_lock(db_mutex);
			rows_changed.fetch_add(FlushBatch(conn, bind_data.target_table, result_batch, bind_data.extract_specs));
		}
	}

	global_state.rows_inserted = rows_changed.load();

	output.SetCardinality(1);
	output.SetValue(0, 0, Value::BIGINT(rows_changed.load()));
}

// Progress callback for CRAWL function - returns fraction 0.0 to 1.0
static double CrawlIntoProgress(ClientContext &context, const FunctionData *bind_data_p,
                                 const GlobalTableFunctionState *gstate_p) {
	if (!gstate_p) {
		return -1.0;  // Unknown progress
	}

	auto &gstate = gstate_p->Cast<CrawlIntoGlobalState>();

	int64_t total = gstate.total_urls.load();
	int64_t processed = gstate.processed_urls.load();

	if (total <= 0) {
		// During discovery phase, show small progress to indicate activity
		if (gstate.discovery_started.load()) {
			return 0.005;  // 0.5% - show "discovering..." progress
		}
		return -1.0;  // Not started yet
	}

	return static_cast<double>(processed) / static_cast<double>(total);
}

void RegisterCrawlIntoFunction(ExtensionLoader &loader) {
	TableFunction crawl_into_func("crawl_into_internal",
	                               {LogicalType::INTEGER,  // statement_type (0=CRAWL)
	                                LogicalType::VARCHAR,  // source_query
	                                LogicalType::VARCHAR,  // target_table
	                                LogicalType::VARCHAR,  // user_agent
	                                LogicalType::DOUBLE,   // default_crawl_delay
	                                LogicalType::DOUBLE,   // min_crawl_delay
	                                LogicalType::DOUBLE,   // max_crawl_delay
	                                LogicalType::INTEGER,  // timeout_seconds
	                                LogicalType::BOOLEAN,  // respect_robots_txt
	                                LogicalType::BOOLEAN,  // log_skipped
	                                LogicalType::VARCHAR,  // url_filter
	                                LogicalType::DOUBLE,   // sitemap_cache_hours
	                                LogicalType::BOOLEAN,  // update_stale
	                                LogicalType::INTEGER,  // max_retry_backoff_seconds
	                                LogicalType::INTEGER,  // max_parallel_per_domain
	                                LogicalType::INTEGER,  // max_total_connections
	                                LogicalType::BIGINT,   // max_response_bytes
	                                LogicalType::BOOLEAN,  // compress
	                                LogicalType::VARCHAR,  // accept_content_types
	                                LogicalType::VARCHAR,  // reject_content_types
	                                LogicalType::BOOLEAN,  // follow_links
	                                LogicalType::BOOLEAN,  // allow_subdomains
	                                LogicalType::INTEGER,  // max_crawl_pages
	                                LogicalType::INTEGER,  // max_crawl_depth
	                                LogicalType::BOOLEAN,  // respect_nofollow
	                                LogicalType::BOOLEAN,  // follow_canonical
	                                LogicalType::INTEGER,  // num_threads
	                                LogicalType::BOOLEAN}, // extract_js
	                               CrawlIntoFunction, CrawlIntoBind, CrawlIntoInitGlobal);

	// Set progress callback for progress bar integration
	crawl_into_func.table_scan_progress = CrawlIntoProgress;

	loader.RegisterFunction(crawl_into_func);
}

} // namespace duckdb
